- Read up again on depth-first traversal.
- Read up on langchain, use it for things.
     - Read up on langgraph as well, and see how well-integrated the two are.


- From the intro of the RAGAS paper... make a good clear section for academic / paper reading to do, divide by section, and by importance, etc etc... so that as I get references to these papers I can clearly add them to my obsidian vault... also, practise reading from, and adding to my obsidian vault, through the github app! this seems like probably the best way to do it, rather than through the obsidian app itself...
    - Basically go through all these relevant links, and organise them around LLM readings, and link each paper under the relevant section, and have links in my vault likely to the paper itself (find out a system using pdfs that stores them in a good way, likely in my onedrive or something?)... and then have each pdf also have notes associated with it.. so there's an obsidian file for each particular paper, and that can be linked around etc...
    - Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any external sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT Devlin et al. (2019) and became more firmly established with the introduction of ever larger LMs Roberts et al. (2020). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering benchmarks Bubeck et al. (2023), the idea of using LLMs as knowledge bases still has two fundamental limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus Kandpal et al. (2022); Mallen et al. (2023). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) Lee et al. (2019); Lewis et al. (2020); Guu et al. (2020). Answering a question then essentially involves retrieving relevant passages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling Khandelwal et al. (2020); Borgeaud et al. (2022), recent work has suggested that simply adding retrieved documents to the input of a standard LM can also work well Khattab et al. (2022); Ram et al. (2023); Shi et al. (2023), thus making it possible to use retrieval-augmented strategies in combination with LLMs that are only available through APIs.
